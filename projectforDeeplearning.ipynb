{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "projectforDeeplearning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Text generation \n",
        "##Mathematics of Deep and Machine Learning Algorithms\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "###By: **Jinghua Duan**\n",
        "\n",
        "\n",
        "> Master 2 - Mathematics and Economic Decision (MED)"
      ],
      "metadata": {
        "id": "FlupLnBDMGt1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1. Text generation with naive LSTM"
      ],
      "metadata": {
        "id": "NjfJjkmV3Z7u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this part, we start to do text generation using O.henry 's 25 short stories and LSTM to generate text. "
      ],
      "metadata": {
        "id": "rs-Z23INKwR_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we start to import some packages"
      ],
      "metadata": {
        "id": "W4c03dhXQWjU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPcjmvicQVVU",
        "outputId": "f3538894-a247-4c3e-b2b2-3f3188b4f73b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import datetime\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import PowerTransformer, MinMaxScaler\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from google.colab import files\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import RNN\n",
        "from keras.utils import np_utils\n",
        "from keras.callbacks import ModelCheckpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Data importation\n",
        "In this project, we choose to load the short stories of O.Henry. The file is a text file that you can download from the data folder on my github:\n",
        "\n",
        "Click on this link : https://github.com/jinghua-duan/Text-generation-with-O-henry-s-short-stories/blob/main/sixes%20and%20sevenes%20ohenry.txt\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "32Lf3L15RCTK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 77
        },
        "id": "nwfMJEFTSnHs",
        "outputId": "1755f9cc-fcfd-4240-c3d5-692a22ede852"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-16a611a6-274d-4b94-93d0-2e26e01e5e1d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-16a611a6-274d-4b94-93d0-2e26e01e5e1d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving sixes and sevenes ohenry.txt to sixes and sevenes ohenry.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = open('sixes and sevenes ohenry.txt','r')\n",
        "ohenry = data.read()\n",
        "data.close()"
      ],
      "metadata": {
        "id": "8wLBtbGXmgQa"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us see the stories."
      ],
      "metadata": {
        "id": "t__nmo2HTOXc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(ohenry[0:1003])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KD_onJJxTJ8D",
        "outputId": "f22df300-3652-4360-8360-da9e2c2164cb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inexorably Sam Galloway saddled his pony. He was going away from the\n",
            "Rancho Altito at the end of a three-months' visit. It is not to be\n",
            "expected that a guest should put up with wheat coffee and biscuits\n",
            "\"yellow-streaked with saleratus for longer than that. Nick Napoleon, the\"\n",
            "\"big Negro man cook, had never been able to make good biscuits: Once\"\n",
            "\"before, when Nick was cooking at the Willow Ranch, Sam had been forced to\"\n",
            "\"fly from his _cuisine_, after only a six-weeks' sojourn.\"\n",
            "\n",
            "\"On Sam's face was an expression of sorrow, deepened with regret and\"\n",
            "slightly tempered by the patient forgiveness of a connoisseur who cannot\n",
            "be understood. But very firmly and inexorably he buckled his\n",
            "\"saddle-cinches, looped his stake-rope and hung it to his saddle-horn, tied\"\n",
            "\"his slicker and coat on the cantle, and looped his quirt on his right\"\n",
            "\"wrist. The Merrydews (householders of the Rancho Altito), men, women,\"\n",
            "\"children, and servants, vassals, visitors, employes, dogs, and casual\"\n",
            "\"callers were grouped i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Text processing"
      ],
      "metadata": {
        "id": "cfY8IgZITafh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this part, we do text processing at character level. First, we need to find all the distinct character and build connection between character and id"
      ],
      "metadata": {
        "id": "Kp0Y3i0YMYiX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "characters = sorted(list(set(ohenry)))\n",
        "\n",
        "n_to_char = {n:char for n, char in enumerate(characters)}\n",
        "char_to_n = {char:n for n, char in enumerate(characters)}\n",
        "\n",
        "vocab_size = len(characters)\n",
        "print('Number of unique characters: ', vocab_size)\n",
        "print(characters)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gm4tgZ8CJvz7",
        "outputId": "5c0aaf49-a05f-4d6e-b50e-225f8a88e3b5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique characters:  81\n",
            "['\\n', ' ', '!', '\"', '#', '$', '&', \"'\", '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(n_to_char)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NWg6fu5oOdz",
        "outputId": "69659e78-5dfb-4564-fc9b-abbd64ab159f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: '\\n', 1: ' ', 2: '!', 3: '\"', 4: '#', 5: '$', 6: '&', 7: \"'\", 8: '(', 9: ')', 10: ',', 11: '-', 12: '.', 13: '0', 14: '1', 15: '2', 16: '3', 17: '4', 18: '5', 19: '6', 20: '7', 21: '8', 22: '9', 23: ':', 24: ';', 25: '?', 26: 'A', 27: 'B', 28: 'C', 29: 'D', 30: 'E', 31: 'F', 32: 'G', 33: 'H', 34: 'I', 35: 'J', 36: 'K', 37: 'L', 38: 'M', 39: 'N', 40: 'O', 41: 'P', 42: 'Q', 43: 'R', 44: 'S', 45: 'T', 46: 'U', 47: 'V', 48: 'W', 49: 'X', 50: 'Y', 51: 'Z', 52: '[', 53: ']', 54: '_', 55: 'a', 56: 'b', 57: 'c', 58: 'd', 59: 'e', 60: 'f', 61: 'g', 62: 'h', 63: 'i', 64: 'j', 65: 'k', 66: 'l', 67: 'm', 68: 'n', 69: 'o', 70: 'p', 71: 'q', 72: 'r', 73: 's', 74: 't', 75: 'u', 76: 'v', 77: 'w', 78: 'x', 79: 'y', 80: 'z'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have 81 unique characters in the document. It contains large and small letters, some separator,punctuation. And we represent each character with a unique number from 0 to 80. "
      ],
      "metadata": {
        "id": "KZLzni0Rk7q6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = []   \n",
        "Y = []  \n",
        "length = len(ohenry)\n",
        "seq_length = 100   \n",
        "\n",
        "for i in range(0, length - seq_length, 1):\n",
        "    sequence = ohenry[i:i + seq_length]\n",
        "    label = ohenry[i + seq_length]\n",
        "    X.append([char_to_n[char] for char in sequence])\n",
        "    Y.append(char_to_n[label])\n",
        "    \n",
        "print('Number of extracted sequences:', len(X))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mTdIt_DKDIX",
        "outputId": "9c11568b-001a-4d77-ce55-d1e54830c283"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of extracted sequences: 356995\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then do some transform of our text and represent it as a vector of numbers.Those number is composed by the id we assigned to each character.Our documents have overall 357095 characters."
      ],
      "metadata": {
        "id": "t3pW9ZmQowLt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "However there is a problem with our text. We did not remove the separator of each line and some html etc. And we will do that in next part."
      ],
      "metadata": {
        "id": "-PX3YLwBl0pu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.shape(Y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0j2qq2CVJIw",
        "outputId": "3716a146-07cd-4a86-87a5-1c90e1f55035"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(356995,)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(Y[0:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5orirz1_rtzi",
        "outputId": "ff9215d2-edba-415d-f732-1c96791d676c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[62, 72, 59, 59, 11, 67, 69, 68, 74, 62, 73, 7, 1, 76, 63, 73, 63, 74, 12, 1, 34, 74, 1, 63, 73, 1, 68, 69, 74, 1, 74, 69, 1, 56, 59, 0, 59, 78, 70, 59, 57, 74, 59, 58, 1, 74, 62, 55, 74, 1, 55, 1, 61, 75, 59, 73, 74, 1, 73, 62, 69, 75, 66, 58, 1, 70, 75, 74, 1, 75, 70, 1, 77, 63, 74, 62, 1, 77, 62, 59, 55, 74, 1, 57, 69, 60, 60, 59, 59, 1, 55, 68, 58, 1, 56, 63, 73, 57, 75, 63]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_modified = np.reshape(X, (len(X), seq_length, 1))\n",
        "X_modified = X_modified / float(len(characters))\n",
        "Y_modified = np_utils.to_categorical(Y)\n",
        "\n",
        "X_modified.shape, Y_modified.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CL8MaT0vKfC-",
        "outputId": "13ef4d05-ca4b-4de6-9a1d-4debeb0bbe54"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((356995, 100, 1), (356995, 81))"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        ">**The shape of our input X**\n",
        "\n",
        "1.   We have 356995 sequences of characters,each sequence has 100 characters. And the depth of each character is 1. It is exactly character level text representation.\n",
        "2. In our model, we predict Y character by character using X. We have 356995 characters in Y and we trans form each character into dummy, then we will have 356995 ✖️ 81 dimension of Y.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "H4VPTvr5nRc7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_modified.shape[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUGDA25zs12B",
        "outputId": "b50dd706-32ce-49ad-b561-9f10ed8365ec"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_modified.shape[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvKigxcMs7vP",
        "outputId": "e6b69f86-441e-4578-c834-de186f6b46ba"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(Y_modified.shape[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTxrf3WHtLyS",
        "outputId": "389b32c1-6d79-4222-df8b-2576e578416b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "81\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 Build LSTM model"
      ],
      "metadata": {
        "id": "jUc-voc7tex1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this part, we start to build our LSTM model. Our model's input is 100✖️1, that means every time we feed our model 100 characters and it gives the output of next character which is a categorical variable."
      ],
      "metadata": {
        "id": "d2v6t51Ttm32"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(400, input_shape=(X_modified.shape[1], X_modified.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(400))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(Y_modified.shape[1], activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "print(model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKDv9Q9jVW43",
        "outputId": "32343452-359c-42a4-adfc-131d03acf5b7"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm (LSTM)                 (None, 100, 400)          643200    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 100, 400)          0         \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 400)               1281600   \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 400)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 81)                32481     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,957,281\n",
            "Trainable params: 1,957,281\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filepath=\"/content/baseline-improvement-ohenry-{epoch:02d}-{loss:.4f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]"
      ],
      "metadata": {
        "id": "41DRVsY1atPu"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_modified, Y_modified, epochs=2, batch_size=2048, callbacks = callbacks_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PWeJllbb4AW",
        "outputId": "1d727dbf-0925-44ad-8b1a-206ae64b1496"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "175/175 [==============================] - ETA: 0s - loss: 3.1518 \n",
            "Epoch 00001: loss improved from inf to 3.15183, saving model to /content/baseline-improvement-ohenry-01-3.1518.hdf5\n",
            "175/175 [==============================] - 9052s 52s/step - loss: 3.1518\n",
            "Epoch 2/2\n",
            "175/175 [==============================] - ETA: 0s - loss: 2.9086 \n",
            "Epoch 00002: loss improved from 3.15183 to 2.90858, saving model to /content/baseline-improvement-ohenry-02-2.9086.hdf5\n",
            "175/175 [==============================] - 9243s 53s/step - loss: 2.9086\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff6b60f4850>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From above, we can see our model performs poorly. The loss is still large and is very computationally expensive. I spend 8 hours to train this model."
      ],
      "metadata": {
        "id": "T96bp7urii9Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4 Generate the text"
      ],
      "metadata": {
        "id": "Q0zn-yTy1vbB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use the model we train before to generate some text.You can download the weight file in my github and load it."
      ],
      "metadata": {
        "id": "JpsI8y1O174b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"/baseline-improvement-ohenry-02-2.9086.hdf5\"\n",
        "model.load_weights(filename)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "metadata": {
        "id": "zZhfrM2Y-uOZ"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = np.random.randint(0, len(X)-1) \n",
        "\n",
        "string_mapped = list(X[start])\n",
        "\n",
        "full_string = [n_to_char[value] for value in string_mapped]\n",
        "\n",
        "print(\"Seed:\")\n",
        "print(\"\\\"\", ''.join(full_string), \"\\\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDuDhiLuHbFk",
        "outputId": "1bbc0870-6d89-4569-a61c-f5d922819a9d"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed:\n",
            "\" seats. Two of them\n",
            "made a fight and were both killed.\n",
            "\n",
            "It took the Daltons just ten minutes to captu \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we select some random sequence of our text with length of sequence as 100 characters. And use it to generate the text."
      ],
      "metadata": {
        "id": "t1dDvb4g-Exi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We just plug in our 100 characters into our model to predict next characters and do it repeatedly until obtaining plausible sequences."
      ],
      "metadata": {
        "id": "wUxFRnhwBn03"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(400):\n",
        "    x = np.reshape(string_mapped,(1,len(string_mapped), 1))\n",
        "    x = x / float(len(characters))\n",
        "\n",
        "    pred_index = np.argmax(model.predict(x, verbose=0))\n",
        "    full_string.append(n_to_char[pred_index])\n",
        "    \n",
        "    string_mapped.append(pred_index)  \n",
        "    string_mapped = string_mapped[1:len(string_mapped)] "
      ],
      "metadata": {
        "id": "XqsVfmLLDbs2"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "txt=\"\"\n",
        "for char in full_string:\n",
        "    txt = txt+char\n",
        "\n",
        "print(start)\n",
        "print(txt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqZgK5a8H1Wj",
        "outputId": "2c0ef29d-724f-41c0-a30e-bbd37fc21c39"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "75892\n",
            "seats. Two of them\n",
            "made a fight and were both killed.\n",
            "\n",
            "It took the Daltons just ten minutes to captu to the toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.4.1 Conclusion"
      ],
      "metadata": {
        "id": "J5zmVYFAjSJQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the results, we can see that our model only generate \"toe\" repeatedly. It makes no sense. Why our model performs so baddly?\n",
        "\n",
        "  \n",
        "\n",
        "1.  one reason is our model is underfitting.\n",
        "2.  Another reason is we didn't do sampling when generating text.\n",
        "3.  Or it is because we didn't do text cleaning.\n",
        "\n"
      ],
      "metadata": {
        "id": "T5wsTF16jfGN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4.2 An alternative input after text processing"
      ],
      "metadata": {
        "id": "xwY9lVYcjWMH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we will clean our original data and build the same model as before to see if it is the reason of model's bad performance."
      ],
      "metadata": {
        "id": "J7REKSDPinpj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "### data cleaning\n",
        "def convert_text_to_lowercase(df):\n",
        "    df = df.lower()\n",
        "    return df\n",
        "    \n",
        "def not_regex(pattern):\n",
        "        return r\"((?!{}).)\".format(pattern)\n",
        "\n",
        "def remove_punctuation(df):\n",
        "    df = df.replace('\\n', ' ')\n",
        "    df = df.replace('\\r', ' ')\n",
        "    alphanumeric_characters_extended = '(\\\\b[-/]\\\\b|[a-zA-Z0-9])'\n",
        "    df = df.replace(not_regex(alphanumeric_characters_extended), ' ')\n",
        "    return df\n",
        "\n",
        "def text_cleaning(df):\n",
        "    \"\"\"\n",
        "    Takes in a string of text, then performs the following:\n",
        "    1. convert text to lowercase\n",
        "    2. remove punctuation and new line characters '\\n'\n",
        "    3. Remove all stopwords\n",
        "    \n",
        "    \"\"\"\n",
        "    df = convert_text_to_lowercase(df)\n",
        "    df = remove_punctuation(df)\n",
        "    \n",
        "    return df"
      ],
      "metadata": {
        "id": "uEpW5BMTil1j"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ohenryclean = text_cleaning(ohenry)\n"
      ],
      "metadata": {
        "id": "N9laNccai3eO"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "characters = sorted(list(set(ohenryclean)))\n",
        "\n",
        "n_to_char = {n:char for n, char in enumerate(characters)}\n",
        "char_to_n = {char:n for n, char in enumerate(characters)}\n",
        "\n",
        "vocab_size = len(characters)\n",
        "print('Number of unique characters: ', vocab_size)\n",
        "print(characters)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYnl8xTGTiXo",
        "outputId": "97de1c6e-7f96-43d7-bf05-4b900c1a9aae"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique characters:  54\n",
            "[' ', '!', '\"', '#', '$', '&', \"'\", '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After cleaning, we only have 54 unique characters."
      ],
      "metadata": {
        "id": "f4F8Eu9L6gmQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = []   \n",
        "Y = []  \n",
        "length = len(ohenryclean)\n",
        "seq_length = 100   \n",
        "\n",
        "for i in range(0, length - seq_length, 1):\n",
        "    sequence = ohenryclean[i:i + seq_length]\n",
        "    label = ohenryclean[i + seq_length]\n",
        "    X.append([char_to_n[char] for char in sequence])\n",
        "    Y.append(char_to_n[label])\n",
        "    \n",
        "print('Number of extracted sequences:', len(X))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXNt2VkXczfu",
        "outputId": "f1cc6169-8f67-48b6-c4c9-4c4302b592f4"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of extracted sequences: 356995\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_modified = np.reshape(X, (len(X), seq_length, 1))\n",
        "X_modified = X_modified / float(len(characters))\n",
        "Y_modified = np_utils.to_categorical(Y)\n",
        "\n",
        "X_modified.shape, Y_modified.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efOUFHb25htx",
        "outputId": "2cda1bad-0e73-4008-fc5e-1467fa57b344"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((356995, 100, 1), (356995, 54))"
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(400, input_shape=(X_modified.shape[1], X_modified.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(400))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(Y_modified.shape[1], activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "metadata": {
        "id": "RpZgcwdo8rVd"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filepath=\"/content/baseline-improvement-ohenry-clean-{epoch:02d}-{loss:.4f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]"
      ],
      "metadata": {
        "id": "XGSkTiTEO9cm"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_modified, Y_modified, epochs=2, batch_size=256, callbacks = callbacks_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4UTdHKaSbLX",
        "outputId": "851e059f-be1e-4bef-de57-0f95a89194b1"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "1395/1395 [==============================] - ETA: 0s - loss: 2.6977\n",
            "Epoch 00001: loss improved from inf to 2.69770, saving model to /content/baseline-improvement-ohenry-clean-01-2.6977.hdf5\n",
            "1395/1395 [==============================] - 9690s 7s/step - loss: 2.6977\n",
            "Epoch 2/2\n",
            "1395/1395 [==============================] - ETA: 0s - loss: 2.4656\n",
            "Epoch 00002: loss improved from 2.69770 to 2.46561, saving model to /content/baseline-improvement-ohenry-clean-02-2.4656.hdf5\n",
            "1395/1395 [==============================] - 9431s 7s/step - loss: 2.4656\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f94e73882d0>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Still, the model is costly in computation. We take **8 hours** to finish training"
      ],
      "metadata": {
        "id": "CSc8bhld6ufl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"/content/baseline-improvement-ohenry-clean-02-2.4656.hdf5\" # link to find the file : https://github.com/DLProjectTextGeneration/TextGeneration/blob/main/Weights/baseline-improvement-britney-clean-40-0.3835_50_128.hdf5\n",
        "model.load_weights(filename)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "metadata": {
        "id": "JSO_F8_qnKCe"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = 40560\n",
        "\n",
        "string_mapped = list(X[start])\n",
        "\n",
        "full_string = [n_to_char[value] for value in string_mapped]\n",
        "\n",
        "print(\"Seed:\")\n",
        "print(\"\\\"\", ''.join(full_string), \"\\\"\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i90a1A4nhpNp",
        "outputId": "58fd5a70-3f2d-4c22-bb03-5189ddc3e334"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed:\n",
            "\" s meagre purchase, but her courage failed\" at the act. she did not dare affront him. she knew the pr \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(400):\n",
        "    x = np.reshape(string_mapped,(1,len(string_mapped), 1))\n",
        "    x = x / float(len(characters))\n",
        "\n",
        "    pred_index = np.argmax(model.predict(x, verbose=0))\n",
        "    full_string.append(n_to_char[pred_index])\n",
        "    \n",
        "    string_mapped.append(pred_index)  \n",
        "    string_mapped = string_mapped[1:len(string_mapped)] "
      ],
      "metadata": {
        "id": "6G5eRrkbXJR-"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "txt=\"\"\n",
        "for char in full_string:\n",
        "    txt = txt+char\n",
        "\n",
        "print(start)\n",
        "print(txt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oaHjFt-phm4O",
        "outputId": "96dc6ede-612b-4e72-bc42-c491a5185db9"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "40560\n",
            "s meagre purchase, but her courage failed\" at the act. she did not dare affront him. she knew the proe of the sooe of the sooe of the sooe of the sooe of the sooe of the sooe of the sooe of the sooe of the sooe of the sooe of the sooe of the sooe of the sooe of the sooe of the sooe of the sooe of the sooe of the sooe of the sooe of the sooe of the sooe of the sooe of the sooe of the sooe of the sooe of the sooe of the sooe of the sooe of the sooe of the sooe of the sooe of the sooe of the sooe o\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, our model only generate some repetitions \"the sooe of\" . It seems that the problems is not about text cleaning. There must be some considerable flaw in you model"
      ],
      "metadata": {
        "id": "Wxtk3-MnEsJy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2. Text generation with correct *LSTM*"
      ],
      "metadata": {
        "id": "ct_8WciQF1br"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Data importation"
      ],
      "metadata": {
        "id": "S3udm0ygHz0C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Start with some importations"
      ],
      "metadata": {
        "id": "IEN9VegQGFfn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "import numpy as np\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import layers, Model\n",
        "import os\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "import string\n",
        "import re"
      ],
      "metadata": {
        "id": "c6uCiTUzF9wy"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_data_ds = tf.data.TextLineDataset([\"/content/sixes and sevenes ohenry.txt\"])"
      ],
      "metadata": {
        "id": "lbqiVVTAGMod"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for elems in raw_data_ds.take(10):\n",
        "    print(elems.numpy().decode(\"utf-8\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBGDX7YVGRDp",
        "outputId": "c3128b3e-fb2a-49f6-c045-76ba75bc9aca"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inexorably Sam Galloway saddled his pony. He was going away from the\n",
            "Rancho Altito at the end of a three-months' visit. It is not to be\n",
            "expected that a guest should put up with wheat coffee and biscuits\n",
            "\"yellow-streaked with saleratus for longer than that. Nick Napoleon, the\"\n",
            "\"big Negro man cook, had never been able to make good biscuits: Once\"\n",
            "\"before, when Nick was cooking at the Willow Ranch, Sam had been forced to\"\n",
            "\"fly from his _cuisine_, after only a six-weeks' sojourn.\"\n",
            "\n",
            "\"On Sam's face was an expression of sorrow, deepened with regret and\"\n",
            "slightly tempered by the patient forgiveness of a connoisseur who cannot\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Text processing "
      ],
      "metadata": {
        "id": "zVnLtx2EIQXp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this part, we will do something different from part 1. We use tensorflow and keras to process the data. First, we concatenate all the text in one line. Thus we can escape from separator and other weird notation.\n"
      ],
      "metadata": {
        "id": "xbSgWrl_Ilyv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"\"\n",
        "for elem in raw_data_ds:\n",
        "   text=text+(elem.numpy().decode('utf-8'))\n",
        "\n",
        "\n",
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-rzBv5_GVud",
        "outputId": "e484ad10-0a8a-42a4-cec0-50f8317bb6d5"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inexorably Sam Galloway saddled his pony. He was going away from theRancho Altito at the end of a three-months' visit. It is not to beexpected that a guest should put up with wheat coffee and biscuits\"yellow-streaked with saleratus for longer than that. Nick Napoleon, the\"\"big Negro man cook, had never been able to make good biscuits: Once\"\"before, when Nick was cooking at the Willow Ranch, Sam had been forced to\"\"fly from his _cuisine_, after only a six-weeks' sojourn.\"\"On Sam's face was an expression of sorrow, deepened with regret and\"slightly tempered by the patient forgiveness of a connoisseur who cannotbe understood. But very firmly and inexorably he buckled his\"saddle-cinches, looped his stake-rope and hung it to his saddle-horn, tied\"\"his slicker and coat on the cantle, and looped his quirt on his right\"\"wrist. The Merrydews (householders of the Rancho Altito), men, women,\"\"children, and servants, vassals, visitors, employes, dogs, and casual\"\"callers were grouped in the \"\"gall\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Corpus length:\", int(len(text)),\"chars\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70-rfUPgGa3X",
        "outputId": "edbece6b-d848-49e4-aae9-c44d8eeccd6d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus length: 350160 chars\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))\n",
        "print(\"Total disctinct chars:\", len(chars))\n",
        "print(chars)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7H1JZp5IGc1D",
        "outputId": "56c7129e-0170-413c-ba76-402f532c9e46"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total disctinct chars: 80\n",
            "[' ', '!', '\"', '#', '$', '&', \"'\", '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have 80 distinct characters and 350160 overall characters in our text. This is because we remove delimiter.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Next we will change the construction of our character sequences.\n",
        "\n",
        "\n",
        "1.   First, we set a shorter character sequence length from 100 to 20.\n",
        "2.   Second, we create next sequence of characters 3 steps after rather than 1 step after.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ypt-esERJsp3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cut the text in semi-redundant sequences of maxlen characters\n",
        "maxlen = 20\n",
        "step = 1\n",
        "input_chars = []\n",
        "next_char = []"
      ],
      "metadata": {
        "id": "M5w6ZJZgGf3f"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0, len(text) - maxlen, step):\n",
        "    input_chars.append(text[i : i + maxlen])\n",
        "    next_char.append(text[i + maxlen])"
      ],
      "metadata": {
        "id": "ldUIn8nXGlEE"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of sequences:\", len(input_chars))\n",
        "print(\"input X  (input_chars)  --->   output y (next_char) \")\n",
        "\n",
        "for i in range(5):\n",
        "  print( input_chars[i],\"   --->  \", next_char[i])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNRQ5TjTGpfu",
        "outputId": "12b99649-171e-46b6-caef-6fdfd254ae25"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sequences: 350140\n",
            "input X  (input_chars)  --->   output y (next_char) \n",
            "Inexorably Sam Gallo    --->   w\n",
            "nexorably Sam Gallow    --->   a\n",
            "exorably Sam Gallowa    --->   y\n",
            "xorably Sam Galloway    --->    \n",
            "orably Sam Galloway     --->   s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_ds_raw=tf.data.Dataset.from_tensor_slices(input_chars)\n",
        "y_train_ds_raw=tf.data.Dataset.from_tensor_slices(next_char)"
      ],
      "metadata": {
        "id": "nqAdXAuNGwfe"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we define some functions to clean the data. Such as transform text into lowercase,remove punctuation, split words etc."
      ],
      "metadata": {
        "id": "pDe0stnbs9xO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_standardization(input_data):\n",
        "    lowercase     = tf.strings.lower(input_data)\n",
        "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
        "    stripped_num  = tf.strings.regex_replace(stripped_html, \"[\\d-]\", \" \")\n",
        "    stripped_punc  =tf.strings.regex_replace(stripped_num, \n",
        "                             \"[%s]\" % re.escape(string.punctuation), \"\")    \n",
        "    return stripped_punc\n",
        "\n",
        "def char_split(input_data):\n",
        "  return tf.strings.unicode_split(input_data, 'UTF-8')\n",
        "\n",
        "def word_split(input_data):\n",
        "  return tf.strings.split(input_data)"
      ],
      "metadata": {
        "id": "z1VCDXCsG3h_"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model constants.\n",
        "batch_size = 64\n",
        "max_features = 96           # Number of distinct chars  \n",
        "embedding_dim = 16             # Embedding layer output dimension\n",
        "sequence_length = maxlen       # Input sequence size\n"
      ],
      "metadata": {
        "id": "ADuUIaPwG7fe"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we choose to use tensorflow to vectorize our data, rather than directly use the id constructed by default."
      ],
      "metadata": {
        "id": "mRLO6w7-wlj8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorize_layer = TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=max_features,\n",
        "    split=char_split, # char_split\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length,\n",
        ")"
      ],
      "metadata": {
        "id": "jCIRMrdDHOMh"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This vectorize_layer `adapt` is basically used on the text-only dataset to create the vocabulary."
      ],
      "metadata": {
        "id": "w_cwv96lhM7f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorize_layer.adapt(X_train_ds_raw.batch(batch_size))"
      ],
      "metadata": {
        "id": "dMHqb5X-HjWu"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The number of distinct characters: \", len(vectorize_layer.get_vocabulary()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GL1uHjZFHlPV",
        "outputId": "c3ff3e26-b49c-4f66-edb1-032edab01a49"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of distinct characters:  29\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The characters are: \", vectorize_layer.get_vocabulary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YV-Z6opHnXu",
        "outputId": "750d7fba-0150-4206-85d1-4c85f429fa81"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The characters are:  ['', '[UNK]', ' ', 'e', 't', 'a', 'o', 'n', 'i', 's', 'h', 'r', 'd', 'l', 'u', 'm', 'w', 'c', 'y', 'f', 'g', 'p', 'b', 'k', 'v', 'j', 'x', 'q', 'z']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "b0LHpbrQhsHl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize_text(text):\n",
        "  text = tf.expand_dims(text, -1)\n",
        "  return tf.squeeze(vectorize_layer(text))"
      ],
      "metadata": {
        "id": "GKLgR1JRTAWG"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorize the data.\n",
        "X_train_ds = X_train_ds_raw.map(vectorize_text)\n",
        "y_train_ds = y_train_ds_raw.map(vectorize_text)\n",
        "\n",
        "X_train_ds.element_spec, y_train_ds.element_spec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJQ7E1x_TEl1",
        "outputId": "0c2967ca-d0e8-474e-de55-88405b1d4dd9"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorSpec(shape=(20,), dtype=tf.int64, name=None),\n",
              " TensorSpec(shape=(20,), dtype=tf.int64, name=None))"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_ds=y_train_ds.map(lambda x: x[0])"
      ],
      "metadata": {
        "id": "zzTdjfOtTK__"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us see the vectorized text"
      ],
      "metadata": {
        "id": "ckYmmRH6vIZc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for (X,y) in zip(X_train_ds.take(5), y_train_ds.take(5)):\n",
        "  print(X.numpy(),\" --> \",y.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFh4Y76i2KVI",
        "outputId": "a9630825-2a09-4b66-9246-d2f658ce904f"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 8  7  3 26  6 11  5 22 13 18  2  9  5 15  2 20  5 13 13  6]  -->  16\n",
            "[ 7  3 26  6 11  5 22 13 18  2  9  5 15  2 20  5 13 13  6 16]  -->  5\n",
            "[ 3 26  6 11  5 22 13 18  2  9  5 15  2 20  5 13 13  6 16  5]  -->  18\n",
            "[26  6 11  5 22 13 18  2  9  5 15  2 20  5 13 13  6 16  5 18]  -->  2\n",
            "[ 6 11  5 22 13 18  2  9  5 15  2 20  5 13 13  6 16  5 18  2]  -->  9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds =  tf.data.Dataset.zip((X_train_ds,y_train_ds))"
      ],
      "metadata": {
        "id": "bz2w_kDevSaU"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "train_ds = train_ds.shuffle(buffer_size=512).batch(batch_size, drop_remainder=True).cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "metadata": {
        "id": "jWkvvLzqFu-j"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sample in train_ds.take(1):\n",
        "  print(\"input (X) dimension: \", sample[0].numpy().shape, \"\\noutput (y) dimension: \",sample[1].numpy().shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W98g1vZuvl7E",
        "outputId": "b3e9b004-0bd7-497c-ad8e-dfdc65dccaad"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input (X) dimension:  (64, 20) \n",
            "output (y) dimension:  (64,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The batch size is 64,thus the input and output dimension is therefore 64✖️20 and 64✖️1"
      ],
      "metadata": {
        "id": "7l5WBXpJvnKI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 Define model"
      ],
      "metadata": {
        "id": "pC2HDVwKw5L9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we define some sampling method to generate text. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "1.   Why do we need sampling?\n",
        "2.   Sampling means randomly picking the next word according to its conditional probability distribution. After generating a probability distribution over vocabulary for the given input sequence, we need to carefully decide how to select the next token from this distribution.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hX0V5adYxxs_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(z):\n",
        "   return np.exp(z)/sum(np.exp(z))\n",
        "def greedy_search(conditional_probability):\n",
        "  return (np.argmax(conditional_probability))\n",
        "def temperature_sampling (conditional_probability, temperature=1.0):\n",
        "  conditional_probability = np.asarray(conditional_probability).astype(\"float64\")\n",
        "  conditional_probability = np.log(conditional_probability) / temperature\n",
        "  reweighted_conditional_probability = softmax(conditional_probability)\n",
        "  probas = np.random.multinomial(1, reweighted_conditional_probability, 1)\n",
        "  return np.argmax(probas)\n",
        "def top_k_sampling(conditional_probability, k):\n",
        "  top_k_probabilities, top_k_indices= tf.math.top_k(conditional_probability, k=k, sorted=True)\n",
        "  top_k_probabilities= np.asarray(top_k_probabilities).astype(\"float32\")\n",
        "  top_k_probabilities= np.squeeze(top_k_probabilities)\n",
        "  top_k_indices = np.asarray(top_k_indices).astype(\"int32\")\n",
        "  top_k_redistributed_probability=softmax(top_k_probabilities)\n",
        "  top_k_redistributed_probability = np.asarray(top_k_redistributed_probability).astype(\"float32\")\n",
        "  sampled_token = np.random.choice(np.squeeze(top_k_indices), p=top_k_redistributed_probability)\n",
        "  return sampled_token"
      ],
      "metadata": {
        "id": "bTEpis_1J2kr"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tf.keras.Input(shape=(sequence_length), dtype=\"int64\")\n",
        "x = layers.Embedding(max_features, embedding_dim)(inputs)\n",
        "x = layers.Dropout(0.2)(x)\n",
        "x = layers.LSTM(512, return_sequences=True)(x)\n",
        "x = layers.Flatten()(x)\n",
        "predictions=  layers.Dense(max_features, activation='softmax')(x)\n",
        "model_LSTM = tf.keras.Model(inputs, predictions,name=\"model_LSTM\")"
      ],
      "metadata": {
        "id": "6yQoNSvMyzgZ"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_LSTM.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model_LSTM.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NY7f3r3Oy2-O",
        "outputId": "dceec158-3b37-4b95-990b-ad785f88527a"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_LSTM\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 20)]              0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, 20, 16)            1536      \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 20, 16)            0         \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 20, 512)           1083392   \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 10240)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 96)                983136    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,068,064\n",
            "Trainable params: 2,068,064\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we mention that we use the different loss function. This is because our output vector is sparse."
      ],
      "metadata": {
        "id": "rrQHy8hdy4k5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_LSTM.fit(train_ds, epochs=20)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puVgZuaFrYQV",
        "outputId": "b668d4d9-c179-4369-c7a3-fa649009b4c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "1823/1823 [==============================] - 700s 383ms/step - loss: 2.7353 - accuracy: 0.2128\n",
            "Epoch 2/20\n",
            "1823/1823 [==============================] - 698s 383ms/step - loss: 2.3920 - accuracy: 0.2909\n",
            "Epoch 3/20\n",
            "1823/1823 [==============================] - 691s 379ms/step - loss: 2.2528 - accuracy: 0.3257\n",
            "Epoch 4/20\n",
            "1823/1823 [==============================] - 696s 382ms/step - loss: 2.1484 - accuracy: 0.3518\n",
            "Epoch 5/20\n",
            "1823/1823 [==============================] - 702s 385ms/step - loss: 2.0493 - accuracy: 0.3764\n",
            "Epoch 6/20\n",
            "1823/1823 [==============================] - 696s 382ms/step - loss: 1.9511 - accuracy: 0.4032\n",
            "Epoch 7/20\n",
            "1823/1823 [==============================] - 700s 384ms/step - loss: 1.8506 - accuracy: 0.4294\n",
            "Epoch 8/20\n",
            "1823/1823 [==============================] - 702s 385ms/step - loss: 1.7483 - accuracy: 0.4591\n",
            "Epoch 9/20\n",
            "1823/1823 [==============================] - 696s 382ms/step - loss: 1.6458 - accuracy: 0.4870\n",
            "Epoch 10/20\n",
            "1823/1823 [==============================] - 696s 382ms/step - loss: 1.5405 - accuracy: 0.5173\n",
            "Epoch 11/20\n",
            "1823/1823 [==============================] - 698s 383ms/step - loss: 1.4387 - accuracy: 0.5464\n",
            "Epoch 12/20\n",
            "1823/1823 [==============================] - 699s 384ms/step - loss: 1.3452 - accuracy: 0.5722\n",
            "Epoch 13/20\n",
            "1823/1823 [==============================] - 697s 382ms/step - loss: 1.2548 - accuracy: 0.5997\n",
            "Epoch 14/20\n",
            "1823/1823 [==============================] - 698s 383ms/step - loss: 1.1733 - accuracy: 0.6228\n",
            "Epoch 15/20\n",
            "1823/1823 [==============================] - 711s 390ms/step - loss: 1.0949 - accuracy: 0.6447\n",
            "Epoch 16/20\n",
            "1823/1823 [==============================] - 711s 390ms/step - loss: 1.0262 - accuracy: 0.6671\n",
            "Epoch 17/20\n",
            "1823/1823 [==============================] - 698s 383ms/step - loss: 0.9641 - accuracy: 0.6846\n",
            "Epoch 18/20\n",
            "1823/1823 [==============================] - 699s 383ms/step - loss: 0.9017 - accuracy: 0.7044\n",
            "Epoch 19/20\n",
            "1823/1823 [==============================] - 700s 384ms/step - loss: 0.8478 - accuracy: 0.7205\n",
            "Epoch 20/20\n",
            "1823/1823 [==============================] - 698s 383ms/step - loss: 0.8012 - accuracy: 0.7348\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7904e02810>"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4 Generate the text"
      ],
      "metadata": {
        "id": "w6NMheT1zPHL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_sequence (encoded_sequence):\n",
        "  deceoded_sequence=[]\n",
        "  for token in encoded_sequence:\n",
        "    deceoded_sequence.append(vectorize_layer.get_vocabulary()[token])\n",
        "  sequence= ''.join(deceoded_sequence)\n",
        "  print(\"\\t\",sequence)\n",
        "  return sequence"
      ],
      "metadata": {
        "id": "6FPcImszzjGr"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, seed_original, step):\n",
        "    seed= vectorize_text(seed_original)\n",
        "    print(\"The prompt is\")\n",
        "    decode_sequence(seed.numpy().squeeze())\n",
        "    \n",
        "\n",
        "    seed= vectorize_text(seed_original).numpy().reshape(1,-1)\n",
        "    #Text Generated by Greedy Search Sampling\n",
        "    generated_greedy_search = (seed)\n",
        "    for i in range(step):\n",
        "      predictions=model.predict(seed)\n",
        "      next_index= greedy_search(predictions.squeeze())\n",
        "      generated_greedy_search = np.append(generated_greedy_search, next_index)\n",
        "      seed= generated_greedy_search[-sequence_length:].reshape(1,sequence_length)\n",
        "    print(\"Text Generated by Greedy Search Sampling:\")\n",
        "    decode_sequence(generated_greedy_search)\n",
        "\n",
        "    #Text Generated by Temperature Sampling\n",
        "    print(\"Text Generated by Temperature Sampling:\")\n",
        "    for temperature in [0.2, 0.5, 1.0, 1.2]:\n",
        "        print(\"\\ttemperature: \", temperature)\n",
        "        seed= vectorize_text(seed_original).numpy().reshape(1,-1)\n",
        "        generated_temperature = (seed)\n",
        "        for i in range(step):\n",
        "            predictions=model.predict(seed)\n",
        "            next_index = temperature_sampling(predictions.squeeze(), temperature)\n",
        "            generated_temperature = np.append(generated_temperature, next_index)\n",
        "            seed= generated_temperature[-sequence_length:].reshape(1,sequence_length)\n",
        "        decode_sequence(generated_temperature)\n",
        "\n",
        "    #Text Generated by Top-K Sampling\n",
        "    print(\"Text Generated by Top-K Sampling:\")\n",
        "    for k in [2, 3, 4, 5]:\n",
        "        print(\"\\tTop-k: \", k)\n",
        "        seed= vectorize_text(seed_original).numpy().reshape(1,-1)\n",
        "        generated_top_k = (seed)\n",
        "        for i in range(step):\n",
        "            predictions=model.predict(seed)\n",
        "            next_index = top_k_sampling(predictions.squeeze(), k)\n",
        "            generated_top_k = np.append(generated_top_k, next_index)\n",
        "            seed= generated_top_k[-sequence_length:].reshape(1,sequence_length)\n",
        "        decode_sequence(generated_top_k)"
      ],
      "metadata": {
        "id": "PAzJZtXLzoau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(model_LSTM,\"he talked to her in a \", 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPEHNdukzesV",
        "outputId": "e23d39ce-8fcb-4a92-ca3b-b67c810d5808"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The prompt is\n",
            "\t he talked to her in \n",
            "Text Generated by Greedy Search Sampling:\n",
            "\t he talked to her in soon for the have of a kinge to a maln coruing out betsime that pieces to had frong edpotiency unt\n",
            "Text Generated by Temperature Sampling:\n",
            "\ttemperature:  0.2\n",
            "\t he talked to her in soon of garest sworld that to me about to a man ffom the molne the capperon a sprank hime and wi\n",
            "\ttemperature:  0.5\n",
            "\t he talked to her in sore doyty wall      of the salinote hoase worside hed noted my lace awas in his rand\n",
            "\ttemperature:  1.0\n",
            "\t he talked to her in dyor for day ecsumllr with a man from the weme a witch gleat now and that badinewimls of to\n",
            "\ttemperature:  1.2\n",
            "\t he talked to her in sheers pkpsive instlads or peh from the seen me wehe and redecen entorated of half never    i\n",
            "Text Generated by Top-K Sampling:\n",
            "\tTop-k:  2\n",
            "\t he talked to her in shewimg sowsly hape that the yabee of a hepertyrar nwert i heard of pyemiled a little sir not an\n",
            "\tTop-k:  3\n",
            "\t he talked to her in somolar on busiellsias wile so fiss ralge ones fartuoosus hadientar poptresis or roudtand  al\n",
            "\tTop-k:  4\n",
            "\t he talked to her in danwer pin i coulddlve mectednamalkiplyssekfto fise  m aldneloow in gaibs w aped celled trach\n",
            "\tTop-k:  5\n",
            "\t he talked to her in syoun oot svalkerr fof fovee exmacl ouct bes tiles   ge trinvly das obt thybolk triingert \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.4.1 Conclusion\n",
        "\n",
        "\n",
        "\n",
        "1.   From above, we can see that our model achieve a relatively small loss, high accuracy and very fast training speed. This is because we use tensor flow to processing data. Besides, we do not use sequential model.\n",
        "2.   And it can produce some meaningful text, not like the first model. Besides, we can also use different sampling method to generate text. \n",
        "\n"
      ],
      "metadata": {
        "id": "ewi31oeq0ig3"
      }
    }
  ]
}